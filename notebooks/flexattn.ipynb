{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55388d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/colligo/project/vlm/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "# os.chdir ..\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1b8b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/colligo/project/vlm/nanoGPT'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615a955",
   "metadata": {},
   "source": [
    "# 1 - Basics for flex attention\n",
    "\n",
    "https://pytorch.org/blog/flexattention/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_dummy_qkv(B, T, n_embd, n_head, device=\"cpu\"):\n",
    "    q = torch.randn(B, n_head, T, n_embd // n_head)\n",
    "    k = torch.randn(B, n_head, T, n_embd // n_head)\n",
    "    v = torch.randn(B, n_head, T, n_embd // n_head)\n",
    "    # also put them on device\n",
    "    q = q.to(device)\n",
    "    k = k.to(device)\n",
    "    v = v.to(device)\n",
    "    return q, k, v\n",
    "\n",
    "B, T, n_embd, n_head = 1, 10, 768, 12\n",
    "q, k, v = get_dummy_qkv(B, T, n_embd, n_head, device=\"cuda\")\n",
    "\n",
    "attn_out = flex_attention(q, k, v, block_mask=block_mask)\n",
    "print (attn_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e15ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 10, 64]) torch.Size([1, 12, 10, 64]) torch.Size([1, 12, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "B, T, n_embd, n_head = 1, 10, 768, 12\n",
    "q, k, v = get_dummy_qkv(B, T, n_embd, n_head)\n",
    "\n",
    "print (q.shape, k.shape, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ac501",
   "metadata": {},
   "source": [
    "## 1.1 - score_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda44bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "def noop(score, b, h, q_idx, kv_idx):\n",
    "    return score\n",
    "\n",
    "from torch.nn.attention.flex_attention import flex_attention\n",
    "\n",
    "attn_out = flex_attention(q, k, v, score_mod=noop)\n",
    "print (attn_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9f6a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "def causal_mask(score, b, h, q_idx, kv_idx):\n",
    "    return torch.where(q_idx >= kv_idx, score, -float(\"inf\"))\n",
    "\n",
    "attn_out = flex_attention(q, k, v, score_mod=causal_mask)\n",
    "print (attn_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c444b3",
   "metadata": {},
   "source": [
    "## 1.2 - Mask Mod\n",
    "\n",
    "While most causal or other mask mods can be implemented using score mod (e.g. by setting value to -inf), it does not leverage sparsity of the mask matrix so is slower. mask_mod better leverages the sparsity of the flex attn mask\n",
    "\n",
    "\n",
    ">However, masking is special compared to other modifications – if something is masked out, we can completely skip its computation! In this case, a causal mask has about 50% sparsity, so not taking advantage of the sparsity would result in a 2x slowdown. Although this score_mod is sufficient to implement causal masking correctly, getting the performance benefits of sparsity requires another concept – mask_mod.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "395c78bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expect q/k/v and block_mask to be on the same device but got cpu and cuda:0.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m q_idx >= kv_idx\n\u001b[32m      6\u001b[39m block_mask = create_block_mask(causal, B=\u001b[38;5;28;01mNone\u001b[39;00m, H=\u001b[38;5;28;01mNone\u001b[39;00m, Q_LEN=\u001b[32m10\u001b[39m, KV_LEN=\u001b[32m10\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m attn_out = \u001b[43mflex_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m (attn_out.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/repa/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1382\u001b[39m, in \u001b[36mflex_attention\u001b[39m\u001b[34m(query, key, value, score_mod, block_mask, scale, enable_gqa, return_lse, kernel_options)\u001b[39m\n\u001b[32m   1379\u001b[39m     scale = \u001b[32m1.0\u001b[39m / math.sqrt(query.size(-\u001b[32m1\u001b[39m))\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query.device != block_mask.kv_num_blocks.device:  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1383\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpect q/k/v and block_mask to be on the same device \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1384\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_mask.kv_num_blocks.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1385\u001b[39m     )\n\u001b[32m   1387\u001b[39m kernel_options = _apply_kernel_options(\n\u001b[32m   1388\u001b[39m     query,\n\u001b[32m   1389\u001b[39m     key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1392\u001b[39m     kernel_options,\n\u001b[32m   1393\u001b[39m )\n\u001b[32m   1395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.compiler.is_dynamo_compiling():\n\u001b[32m   1396\u001b[39m     \u001b[38;5;66;03m# mark head_dim and number of heads to be static\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expect q/k/v and block_mask to be on the same device but got cpu and cuda:0."
     ]
    }
   ],
   "source": [
    "from torch.nn.attention.flex_attention import create_block_mask\n",
    "\n",
    "def causal(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "block_mask = create_block_mask(causal, B=None, H=None, Q_LEN=10, KV_LEN=10)\n",
    "\n",
    "attn_out = flex_attention(q, k, v, block_mask=block_mask)\n",
    "print (attn_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d59a2a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockMask(\n",
       "    kv_num_blocks=torch.Size([1, 1, 1]),\n",
       "    kv_indices=torch.Size([1, 1, 1, 1]),\n",
       "    full_kv_num_blocks=torch.Size([1, 1, 1]),\n",
       "    full_kv_indices=torch.Size([1, 1, 1, 1]),\n",
       "    q_num_blocks=torch.Size([1, 1, 1]),\n",
       "    q_indices=torch.Size([1, 1, 1, 1]),\n",
       "    full_q_num_blocks=torch.Size([1, 1, 1]),\n",
       "    full_q_indices=torch.Size([1, 1, 1, 1]),\n",
       "    BLOCK_SIZE=(128, 128),\n",
       "    shape=(1, 1, 10, 10),\n",
       "    sparsity=-16284.00%,\n",
       "    mask_mod=causal\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701164cb",
   "metadata": {},
   "source": [
    "# 2 - Flex attn for gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a8569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919e4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b040f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6145466",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fde10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
